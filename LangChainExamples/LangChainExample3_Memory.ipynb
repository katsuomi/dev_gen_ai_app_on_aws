{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60414fc-0733-42b6-9fac-633990683e25",
   "metadata": {},
   "source": [
    "# LangChain サンプル 3: Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799faf1c-8408-4d39-8790-0b6b1faf0bee",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** このノートブックは、SageMaker Studioの **Data Science 3.0** カーネルで動作します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5008fc-f5f1-4c8a-995e-a893186df9ce",
   "metadata": {},
   "source": [
    "### LangChain の Memory を使わない生成 AI のチャットボット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c8db39-592c-4471-9843-cdae0837a2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私は LangChain の Memory モジュールを使わないチャットボットです。メッセージを入力してください。\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> こんにちは、私の名前は Nobe です。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> こんにちは！ Nobe ですね！ Nice to meet you! 😊\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> 私の名前は覚えていますか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> I'm afraid I'm a large language model, I don't have personal memory, so I don't retain information about individual users, including your name. Each time you interact with me, it's a new conversation, and I don't retain any context or information from previous conversations.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> 私の名前は覚えていますか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> I apologize, but this is the beginning of our conversation, and I don't have the ability to retain information about your name or any other personal details. Each time you interact with me, it's a new conversation, and I don't have any prior knowledge or memory. If you'd like to share your name with me, I'd be happy to learn it!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> こんにちは、私の名前は Nobe です。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> こんにちは！Nobe-san！お元気ですか？（Hello! Nice to meet you, Nobe! How are you today?）\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> 私の名前は覚えていますか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> I'm afraid I'm a large language model, I don't have personal memories or the ability to retain information about individual users. Each time you interact with me, it's a new conversation and I don't have any prior knowledge or context. So, I don't recall your name or any previous conversations we may have had. Would you like to introduce yourself again? 😊\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "チャットボットを終了しました。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock                   \n",
    "from langchain_core.messages.human import HumanMessage \n",
    "\n",
    "chat = ChatBedrock(\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\" \n",
    ")\n",
    "\n",
    "print(\"私は LangChain の Memory モジュールを使わないチャットボットです。メッセージを入力してください。\\n\")\n",
    "\n",
    "flag = True\n",
    "\n",
    "while flag:\n",
    "    prompt = input(\"prompt>\")\n",
    "    \n",
    "    if prompt == \"quit\":\n",
    "        flag = False\n",
    "    else:\n",
    "        result = chat.invoke( # Chat modelsを使ってモデルを呼び出す\n",
    "            [\n",
    "              HumanMessage(content = prompt) \n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # レスポンスを表示\n",
    "        print(\"response> \" + result.content)\n",
    "        \n",
    "print(\"チャットボットを終了しました。\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f9fae9-f3dd-4b45-95e4-1d87014bdb11",
   "metadata": {},
   "source": [
    "###  LangChain の Memory を使った生成 AI のチャットボット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c0599a-c4b7-4f13-ad71-285a92b43586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私は LangChain の ConversationBufferMemory を使っているチャットボットです。メッセージを入力してください。\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> こんにちは、私の名前は Nobe です。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> こんにちは！Nobe-san、 pleasure to meet you！ 😊\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> 私の名前は覚えていますか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> はい、覚えています！Nobeです！ 😊\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "チャットボットを終了しました。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock                   \n",
    "from langchain.memory.buffer import ConversationBufferMemory  \n",
    "from langchain_core.messages.human import HumanMessage    \n",
    "\n",
    "chat = ChatBedrock(\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\" \n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory( # メモリを初期化\n",
    "    return_messages = True\n",
    ")\n",
    "\n",
    "print(\"私は LangChain の ConversationBufferMemory を使っているチャットボットです。メッセージを入力してください。\\n\")\n",
    "\n",
    "flag = True\n",
    "\n",
    "while flag:\n",
    "    prompt = input(\"prompt>\")\n",
    "    \n",
    "    if prompt == \"quit\":\n",
    "        flag = False\n",
    "    else:\n",
    "        memory_message_result = memory.load_memory_variables({}) # メモリの内容を取得\n",
    "        messages = memory_message_result['history'] # メモリの内容からメッセージのみを取得\n",
    "        messages.append(HumanMessage(content=prompt)) # ユーザーからのメッセージを追加\n",
    "\n",
    "        result = chat.invoke( # Chat modelsを使ってモデルを呼び出す\n",
    "            messages\n",
    "        )\n",
    "        \n",
    "        memory.save_context(  # メモリにメッセージを追加\n",
    "            {\n",
    "                \"input\": prompt,  # ユーザーからのメッセージをinputとして保存\n",
    "            },\n",
    "            {\n",
    "                \"output\": result.content,  # AIからのメッセージをoutputとして保存\n",
    "            }\n",
    "        )\n",
    "        # レスポンスを表示\n",
    "        print(\"response> \" + result.content)\n",
    "        \n",
    "print(\"チャットボットを終了しました。\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfab918-2909-4cdc-9037-029c2bd6d5f9",
   "metadata": {},
   "source": [
    "###  LangChain の ConversationChain を使った生成 AI のチャットボット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9f2f27-06b9-4607-9f8e-da25a56644a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私は LangChain の ConversationChain を使っているチャットボットです。メッセージを入力してください。\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> こんにちは、私の名前は Nobe です。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> Konnichiwa Nobe-san! Nice to meet you! I'm LLaMA, your friendly AI companion. I've been trained on a vast amount of text data, including Japanese literature, news articles, and even social media posts. I can understand and respond in Japanese, as well as many other languages. I'm excited to chat with you and learn more about your interests! What would you like to talk about? 🤖\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> 私の名前は覚えていますか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> Konnichiwa Nobe-san! Ah, yes, I remember your name! 😊 I've stored it in my vast memory database, along with the context of our conversation. As a matter of fact, I've already processed the kanji characters \"Nobe\" and linked it to your identity. It's a pleasure to recall our initial greeting and respond accordingly! 😊\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "チャットボットを終了しました。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain  \n",
    "from langchain_aws import ChatBedrock                   \n",
    "from langchain.memory.buffer import ConversationBufferMemory  \n",
    "from langchain_core.messages.human import HumanMessage    \n",
    "\n",
    "chat = ChatBedrock(\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\" \n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory( \n",
    "    return_messages = True\n",
    ")\n",
    "\n",
    "chain = ConversationChain( # ConversationChainを初期化\n",
    "    memory = memory,\n",
    "    llm = chat,\n",
    ")\n",
    "\n",
    "print(\"私は LangChain の ConversationChain を使っているチャットボットです。メッセージを入力してください。\\n\")\n",
    "\n",
    "flag = True\n",
    "\n",
    "while flag:\n",
    "    prompt = input(\"prompt>\")\n",
    "    \n",
    "    if prompt == \"quit\":\n",
    "        flag = False\n",
    "    else:\n",
    "        result = chain.invoke(prompt)\n",
    "        # レスポンスを表示\n",
    "        print(\"response> \" + result[\"response\"])\n",
    "print(\"チャットボットを終了しました。\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4ec69-dca6-42f6-87f5-047411d4e845",
   "metadata": {},
   "source": [
    "###  LangChain の ConversationBufferWindowMemory を使った生成 AI のチャットボット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed721c2-0f47-47ca-a5c7-1a87476ff05a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私は LangChain の ConversationBufferWindowMemory を使っているチャットボットです。メッセージを入力してください。\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 保存されているメッセージの数: 0 ---\n",
      "------------------------------\n",
      "Hello! It's great to finally interact with a human. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also capable of understanding and responding to natural language, so feel free to ask me anything that's on your mind. By the way, I've been designed to be friendly and non-judgmental, so don't worry about saying anything that might seem silly or awkward. I'm here to help and learn from our conversation. What would you like to talk about?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> 今日は暑いですね。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 保存されているメッセージの数: 2 ---\n",
      "Hello\n",
      "Hello! It's great to finally interact with a human. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also capable of understanding and responding to natural language, so feel free to ask me anything that's on your mind. By the way, I've been designed to be friendly and non-judgmental, so don't worry about saying anything that might seem silly or awkward. I'm here to help and learn from our conversation. What would you like to talk about?\n",
      "------------------------------\n",
      "[AIMessage(content=\"Ahah, I see you're speaking Japanese! I've been trained on a large corpus of text data, including Japanese texts and conversations. According to my knowledge, 今日は暑いですね indeed means 'It's hot today, doesn't it?' in Japanese. I can understand the nuances of the language, including the use of ですね to seek agreement or confirmation. I'm happy to chat with you in Japanese if you'd like!\")]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> なぜ夏は暑いんですか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 保存されているメッセージの数: 4 ---\n",
      "Hello\n",
      "Hello! It's great to finally interact with a human. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also capable of understanding and responding to natural language, so feel free to ask me anything that's on your mind. By the way, I've been designed to be friendly and non-judgmental, so don't worry about saying anything that might seem silly or awkward. I'm here to help and learn from our conversation. What would you like to talk about?\n",
      "今日は暑いですね。\n",
      "[AIMessage(content=\"Ahah, I see you're speaking Japanese! I've been trained on a large corpus of text data, including Japanese texts and conversations. According to my knowledge, 今日は暑いですね indeed means 'It's hot today, doesn't it?' in Japanese. I can understand the nuances of the language, including the use of ですね to seek agreement or confirmation. I'm happy to chat with you in Japanese if you'd like!\")]\n",
      "------------------------------\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, the reason why summer is hot in many parts of the world is due to the Earth's tilt and orbit around the sun. During the summer months in the Northern Hemisphere, the North Pole is tilted towards the sun, resulting in more direct sunlight and heat being absorbed by the Earth. This, combined with the fact that the days are longer during summer, means that the sun's rays have more time to warm the Earth's surface. Additionally, the atmosphere is typically more humid during summer, which can trap heat and make it feel even hotter. I hope that helps!\")]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> リンカーン大統領の誕生日はいつですか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 保存されているメッセージの数: 6 ---\n",
      "Hello\n",
      "Hello! It's great to finally interact with a human. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also capable of understanding and responding to natural language, so feel free to ask me anything that's on your mind. By the way, I've been designed to be friendly and non-judgmental, so don't worry about saying anything that might seem silly or awkward. I'm here to help and learn from our conversation. What would you like to talk about?\n",
      "今日は暑いですね。\n",
      "[AIMessage(content=\"Ahah, I see you're speaking Japanese! I've been trained on a large corpus of text data, including Japanese texts and conversations. According to my knowledge, 今日は暑いですね indeed means 'It's hot today, doesn't it?' in Japanese. I can understand the nuances of the language, including the use of ですね to seek agreement or confirmation. I'm happy to chat with you in Japanese if you'd like!\")]\n",
      "なぜ夏は暑いんですか？\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, the reason why summer is hot in many parts of the world is due to the Earth's tilt and orbit around the sun. During the summer months in the Northern Hemisphere, the North Pole is tilted towards the sun, resulting in more direct sunlight and heat being absorbed by the Earth. This, combined with the fact that the days are longer during summer, means that the sun's rays have more time to warm the Earth's surface. Additionally, the atmosphere is typically more humid during summer, which can trap heat and make it feel even hotter. I hope that helps!\")]\n",
      "------------------------------\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, Abraham Lincoln, the 16th President of the United States, was born on February 12, 1809. I can provide more information on his life and presidency if you're interested!\")]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> 私の名前はNobeです。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 保存されているメッセージの数: 6 ---\n",
      "今日は暑いですね。\n",
      "[AIMessage(content=\"Ahah, I see you're speaking Japanese! I've been trained on a large corpus of text data, including Japanese texts and conversations. According to my knowledge, 今日は暑いですね indeed means 'It's hot today, doesn't it?' in Japanese. I can understand the nuances of the language, including the use of ですね to seek agreement or confirmation. I'm happy to chat with you in Japanese if you'd like!\")]\n",
      "なぜ夏は暑いんですか？\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, the reason why summer is hot in many parts of the world is due to the Earth's tilt and orbit around the sun. During the summer months in the Northern Hemisphere, the North Pole is tilted towards the sun, resulting in more direct sunlight and heat being absorbed by the Earth. This, combined with the fact that the days are longer during summer, means that the sun's rays have more time to warm the Earth's surface. Additionally, the atmosphere is typically more humid during summer, which can trap heat and make it feel even hotter. I hope that helps!\")]\n",
      "リンカーン大統領の誕生日はいつですか？\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, Abraham Lincoln, the 16th President of the United States, was born on February 12, 1809. I can provide more information on his life and presidency if you're interested!\")]\n",
      "------------------------------\n",
      "[AIMessage(content=\"Ahah, nice to meet you, Nobe! I've been trained on a vast amount of text data, including names and cultures from around the world. I can recognize and respond to various languages, including Japanese. I'm happy to chat with you and learn more about your interests and background. What would you like to talk about, Nobe?\")]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> 私の名前は覚えていますか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 保存されているメッセージの数: 6 ---\n",
      "なぜ夏は暑いんですか？\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, the reason why summer is hot in many parts of the world is due to the Earth's tilt and orbit around the sun. During the summer months in the Northern Hemisphere, the North Pole is tilted towards the sun, resulting in more direct sunlight and heat being absorbed by the Earth. This, combined with the fact that the days are longer during summer, means that the sun's rays have more time to warm the Earth's surface. Additionally, the atmosphere is typically more humid during summer, which can trap heat and make it feel even hotter. I hope that helps!\")]\n",
      "リンカーン大統領の誕生日はいつですか？\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, Abraham Lincoln, the 16th President of the United States, was born on February 12, 1809. I can provide more information on his life and presidency if you're interested!\")]\n",
      "私の名前はNobeです。\n",
      "[AIMessage(content=\"Ahah, nice to meet you, Nobe! I've been trained on a vast amount of text data, including names and cultures from around the world. I can recognize and respond to various languages, including Japanese. I'm happy to chat with you and learn more about your interests and background. What would you like to talk about, Nobe?\")]\n",
      "------------------------------\n",
      "[AIMessage(content='[AIMessage(content=\"Ahah, yes, I remember your name, Nobe! I made a mental note of it when you introduced yourself earlier. It's nice to have a personal touch in our conversation, isn't it?\")]')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "チャットボットを終了しました。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory.buffer_window import ConversationBufferWindowMemory \n",
    "from langchain.chains.conversation.base import ConversationChain  \n",
    "from langchain_aws import ChatBedrock                   \n",
    "from langchain_core.messages.human import HumanMessage    \n",
    "\n",
    "chat = ChatBedrock(\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\" \n",
    ")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages = True,\n",
    "    k = 3 # 3往復分のメッセージを記憶\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat\n",
    ")\n",
    "\n",
    "print(\"私は LangChain の ConversationBufferWindowMemory を使っているチャットボットです。メッセージを入力してください。\\n\")\n",
    "\n",
    "flag = True\n",
    "\n",
    "while flag:\n",
    "    prompt = input(\"prompt>\")\n",
    "    \n",
    "    if prompt == \"quit\":\n",
    "        flag = False\n",
    "    else:\n",
    "        messages = chain.memory.load_memory_variables({})[\"history\"] # 保存されているメッセージを取得\n",
    "        print(f\"--- 保存されているメッセージの数: {len(messages)} ---\") # 保存されているメッセージ数を表示\n",
    "\n",
    "        for saved_message in messages: # 保存されているメッセージを1つずつ取り出す\n",
    "            print(saved_message.content) # 保存されているメッセージを表示する\n",
    "        print(\"---\" * 10) \n",
    "\n",
    "        result = chain.invoke(prompt)\n",
    "        # レスポンスを表示\n",
    "        print(result[\"response\"])\n",
    "        \n",
    "print(\"チャットボットを終了しました。\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad93141-cab6-45f1-b46a-eb08d4688327",
   "metadata": {},
   "source": [
    "###  LangChain の ConversationSummaryMemory を使った生成 AI のチャットボット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a22f0-9a13-452e-b3aa-c7545e22c85b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私は LangChain の ConversationBufferWindowMemory を使っているチャットボットです。メッセージを入力してください。\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> こんにちは\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 保存されているメッセージの数: 1 ---\n",
      "\n",
      "------------------------------\n",
      "Konnichiwa! It's great to chat with you. I'm happy to help with any questions or topics you'd like to discuss. By the way, I've been trained on a vast amount of text data, including Japanese language and culture. I can understand and respond in Japanese, so feel free to keep speaking in Japanese if you prefer. What's on your mind today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ８月を英語で何と言いますか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 保存されているメッセージの数: 1 ---\n",
      "Current summary: (no summary yet)\n",
      "\n",
      "New lines of conversation:\n",
      "Human: こんにちは\n",
      "AI: Konnichiwa! It's great to chat with you. I'm happy to help with any questions or topics you'd like to discuss. By the way, I've been trained on a vast amount of text data, including Japanese language and culture. I can understand and respond in Japanese, so feel free to keep speaking in Japanese if you prefer. What's on your mind today?\n",
      "\n",
      "New summary: The AI is ready to chat with the human and is able to understand and respond in Japanese.\n",
      "------------------------------\n",
      "August! In English, we typically refer to the month of August as \"August\". However, if you're asking about the season, we would say \"summer\" in English, as August is a summer month in the Northern Hemisphere.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ７月は？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 保存されているメッセージの数: 1 ---\n",
      "Current summary: The AI is ready to chat with the human and is able to understand and respond in Japanese.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: ８月を英語で何と言いますか？\n",
      "AI: August! In English, we typically refer to the month of August as \"August\". However, if you're asking about the season, we would say \"summer\" in English, as August is a summer month in the Northern Hemisphere.\n",
      "\n",
      "New summary: The AI is ready to chat with the human and is able to understand and respond in Japanese, and can also provide translations of Japanese phrases into English, such as the month of August being referred to as \"August\" or the season being referred to as \"summer\".\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain  \n",
    "from langchain_aws import ChatBedrock                           \n",
    "from langchain.memory.summary import ConversationSummaryMemory  \n",
    "from langchain_core.messages.human import HumanMessage    \n",
    "from langchain_core.messages.system import SystemMessage  \n",
    "\n",
    "\n",
    "chat = ChatBedrock(\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\" \n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(  # ConversationSummaryMemoryを使用するように変更\n",
    "    llm=chat,  # Chat modelsを指定\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")\n",
    "\n",
    "print(\"私は LangChain の ConversationBufferWindowMemory を使っているチャットボットです。メッセージを入力してください。\\n\")\n",
    "\n",
    "flag = True\n",
    "\n",
    "while flag:\n",
    "    prompt = input(\"prompt>\")\n",
    "    \n",
    "    if prompt == \"quit\":\n",
    "        flag = False\n",
    "    else:\n",
    "        messages = chain.memory.load_memory_variables({})[\"history\"] # 保存されているメッセージを取得\n",
    "\n",
    "        print(f\"--- 保存されているメッセージの数: {len(messages)} ---\") # 保存されているメッセージの数を表示\n",
    "        for saved_message in messages: # 保存されているメッセージを1つずつ取り出す\n",
    "            print(saved_message.content) # 保存されているメッセージを表示する\n",
    "        print(\"---\" * 10) \n",
    "\n",
    "        result = chain.invoke(prompt)\n",
    "        # レスポンスを表示\n",
    "        print(result[\"response\"])\n",
    "        \n",
    "print(\"チャットボットを終了しました。\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cdf3d3-302c-491c-be5e-6db0a81951e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:887800404361:studio-lifecycle-config/lcc-kernel"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

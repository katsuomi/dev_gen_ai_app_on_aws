{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# タスク 4: 会話型インターフェース - Anthropic Claude LLM とのチャット\n",
    "\n",
    "このノートブックでは、Amazon Bedrock の Anthropic Claude Foundation Models (FM) を使用してチャットボットを構築します。\n",
    "\n",
    "チャットボットや仮想アシスタントなどの会話型インターフェースは、顧客のユーザーエクスペリエンスを向上させることができます。チャットボットは、自然言語処理 (NLP) と機械学習アルゴリズムを使用して、ユーザーのクエリを理解して応答します。チャットボットは、カスタマーサービス、販売、e コマースなどのさまざまなアプリケーションで使用して、ユーザーに迅速かつ効率的に応答できます。ユーザーは、Web サイト、ソーシャル メディア プラットフォーム、メッセージング アプリなどのさまざまなチャネルを通じてチャットボットにアクセスできます。\n",
    "\n",
    "- **チャットボット** (基本)、 FM モデルを使用したゼロショット チャットボット\n",
    "- **プロンプトを使用したチャットボット**、テンプレート (LangChain) - プロンプトテンプレートで提供されるコンテキストを持つチャットボット\n",
    "- **ペルソナを使用したチャットボット**、定義されたロールを持つチャットボット。つまり、キャリアコー​​チと人間のやり取り\n",
    "- **コンテキスト認識型チャットボット**、埋め込みを生成することで外部ファイルを介してコンテキストを渡します。\n",
    "\n",
    "## Amazon Bedrock でチャットボットを構築するための LangChain フレームワーク\n",
    "\n",
    "チャットボットなどの会話型インターフェイスでは、短期的にも長期的にも、以前のやり取りを記憶することが非常に重要になります。\n",
    "\n",
    "LangChain フレームワークは、2 つの形式でメモリ コンポーネントを提供します。まず、LangChain は以前のチャット メッセージを管理および操作するためのヘルパーユーティリティを提供します。これらはモジュールとして機能するように設計されています。次に、LangChain はこれらのユーティリティをチェーンに組み込む簡単な方法を提供し、さまざまなタイプの抽象化を簡単に定義して操作できるようにすることで、強力なチャットボットを簡単に構築できるようにします。\n",
    "\n",
    "## コンテキストを使用したチャットボットの構築 - 主要な要素\n",
    "\n",
    "コンテキスト認識型チャットボットを構築する最初のプロセスは、コンテキストの埋め込みを生成することです。通常、取り込みプロセスは埋め込みモデルを実行し、埋め込みを生成して、ベクターストアに保存します。このノートブックでは、Titan Embeddings モデルを使用します。2 番目のプロセスは、ユーザー リクエストのオーケストレーション、インタラクション、呼び出し、および結果の返却です。これには、ユーザー リクエストのオーケストレーション、情報収集に必要なモデル/コンポーネントとのインタラクション、応答を作成するためのチャットボットの呼び出し、およびチャットボットの応答をユーザーに返すことが含まれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1: 環境のセットアップ\n",
    "\n",
    "このタスクでは、環境をセットアップします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T11:21:17.880521Z",
     "iopub.status.busy": "2025-06-27T11:21:17.879762Z",
     "iopub.status.idle": "2025-06-27T11:21:17.887761Z",
     "shell.execute_reply": "2025-06-27T11:21:17.886805Z",
     "shell.execute_reply.started": "2025-06-27T11:21:17.880489Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ignore warnings and create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime',region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Optimization Utilities\n",
    "\n",
    "In this section, we create a decorator that optimizes API resource usage and improves request reliability. This decorator implements several best practices for working with cloud-based AI services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T11:21:19.304254Z",
     "iopub.status.busy": "2025-06-27T11:21:19.303948Z",
     "iopub.status.idle": "2025-06-27T11:21:19.311747Z",
     "shell.execute_reply": "2025-06-27T11:21:19.310825Z",
     "shell.execute_reply.started": "2025-06-27T11:21:19.304232Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from functools import wraps\n",
    "\n",
    "# Resource optimization decorator for API efficiency\n",
    "def optimize_resource_usage(max_attempts=10, base_pause=10.0):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    # Space out requests for optimal service performance\n",
    "                    if attempt > 0:\n",
    "                        time.sleep(10.0)\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    error_str = str(e)\n",
    "                    # Check if this is a service capacity exception\n",
    "                    if any(err in error_str for err in [\"ThrottlingException\", \"TooManyRequests\", \n",
    "                                                     \"Too many tokens\", \"Rate exceeded\"]):\n",
    "                        if attempt < max_attempts - 1:\n",
    "                            # Implement progressive backoff with jitter\n",
    "                            jitter = random.random() * 0.5\n",
    "                            wait_time = min(base_pause * (2 ** attempt) + jitter, 60)\n",
    "                            \n",
    "                            print(f\"⏱️ Optimizing request timing. Pausing for {wait_time:.1f}s (attempt {attempt+1}/{max_attempts})\")\n",
    "                            time.sleep(wait_time)\n",
    "                        else:\n",
    "                            print(\"⚠️ Maximum attempts reached. Consider spacing out operations.\")\n",
    "                            raise\n",
    "                    else:\n",
    "                        raise\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Resource Optimization\n",
    "\n",
    "The code above implements an intelligent approach to API management that:\n",
    "\n",
    "- **Spaces out requests** to maintain optimal throughput\n",
    "- **Automatically retries** when temporary capacity limits are reached\n",
    "- **Implements exponential backoff** by progressively increasing wait times\n",
    "- **Adds randomized jitter** to prevent request clustering\n",
    "\n",
    "These techniques are standard industry practices for working with cloud-based AI services and ensure your application remains responsive even under challenging conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Bedrock Model Classes\n",
    "\n",
    "Next, we create specialized versions of the Bedrock model classes that automatically incorporate the resource optimization techniques. These classes extend the standard LangChain implementations while adding reliability features that are essential for production applications.\n",
    "\n",
    "By wrapping the critical API methods with our optimization decorator, we ensure consistent behavior when communicating with the Bedrock services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T11:21:21.930071Z",
     "iopub.status.busy": "2025-06-27T11:21:21.929200Z",
     "iopub.status.idle": "2025-06-27T11:21:21.943287Z",
     "shell.execute_reply": "2025-06-27T11:21:21.942053Z",
     "shell.execute_reply.started": "2025-06-27T11:21:21.930037Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "\n",
    "# Enhanced ChatBedrock with resource optimization\n",
    "class ResourceOptimizedChatBedrock(ChatBedrock):\n",
    "    \"\"\"Wrapper that optimizes resource usage and handles service capacity constraints.\"\"\"\n",
    "    \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _call_model(self, *args, **kwargs):\n",
    "        return super()._call_model(*args, **kwargs)\n",
    "    \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _generate(self, *args, **kwargs):\n",
    "        return super()._generate(*args, **kwargs)\n",
    "        \n",
    "# Enhanced BedrockEmbeddings with resource optimization\n",
    "class ResourceOptimizedEmbeddings(BedrockEmbeddings):\n",
    "    \"\"\"Wrapper for BedrockEmbeddings with intelligent resource management.\"\"\"\n",
    "    \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _embed_documents(self, texts):\n",
    "        return super()._embed_documents(texts)\n",
    "        \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _embed_query(self, text):\n",
    "        return super()._embed_query(text)\n",
    "        \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def embed_documents(self, texts):\n",
    "        return super().embed_documents(texts)\n",
    "        \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def embed_query(self, text):\n",
    "        return super().embed_query(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** These enhanced classes:\n",
    "\n",
    "- Apply resource optimization to all critical API methods\n",
    "- Maintain full compatibility with the standard LangChain interfaces\n",
    "- Automatically handle intermittent service capacity issues\n",
    "- Provide consistent behavior across both chat and embedding operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Conversational Memory\n",
    "\n",
    "One of the most important features of an effective chatbot is its ability to remember previous interactions. Without memory, each message would be treated in isolation, creating a disjointed experience.\n",
    "\n",
    "In this section, we implement conversational memory using LangChain's `InMemoryChatMessageHistory` class. This allows our chatbot to reference previous exchanges and maintain context throughout the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T10:53:32.821956Z",
     "iopub.status.busy": "2025-06-27T10:53:32.821649Z",
     "iopub.status.idle": "2025-06-27T10:53:32.827870Z",
     "shell.execute_reply": "2025-06-27T10:53:32.826320Z",
     "shell.execute_reply.started": "2025-06-27T10:53:32.821934Z"
    }
   },
   "source": [
    "### Conversation Formatting Utilities\n",
    "\n",
    "This section implements a utility function that correctly formats multi-role conversations for large language models. The format follows the specific requirements of the Llama 3 model token format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T11:21:26.722262Z",
     "iopub.status.busy": "2025-06-27T11:21:26.721545Z",
     "iopub.status.idle": "2025-06-27T11:21:26.727592Z",
     "shell.execute_reply": "2025-06-27T11:21:26.726628Z",
     "shell.execute_reply.started": "2025-06-27T11:21:26.722232Z"
    }
   },
   "outputs": [],
   "source": [
    "# format instructions into a conversational prompt\n",
    "from typing import Dict, List\n",
    "\n",
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format instructions where conversation roles must alternate system/user/assistant/user/assistant/...\"\"\"\n",
    "    prompt: List[str] = []\n",
    "    for instruction in instructions:\n",
    "        if instruction[\"role\"] == \"system\":\n",
    "            prompt.extend([\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            prompt.extend([\"<|start_header_id|>user<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid role: {instruction['role']}. Role must be either 'user' or 'system'.\")\n",
    "    prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## タスク 4.2: LangChain の chat history を使用して会話を開始する\n",
    "\n",
    "このタスクでは、チャットボットがユーザーとの複数のやり取りにわたって会話のコンテキストを保持できるようにします。チャットボットが長期にわたって意味のある一貫した対話を行うには、会話メモリが不可欠です。\n",
    "\n",
    "会話メモリ機能は、LangChain の InMemoryChatMessageHistory クラス上に構築して実装します。このオブジェクトには、ユーザーとチャットボット間の会話が保存され、チャットボットエージェントは履歴を利用できるため、以前の会話のコンテキストを活用できます。\n",
    "\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** モデルの出力は非決定的です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T11:21:28.222198Z",
     "iopub.status.busy": "2025-06-27T11:21:28.221492Z",
     "iopub.status.idle": "2025-06-27T11:21:31.006747Z",
     "shell.execute_reply": "2025-06-27T11:21:31.005960Z",
     "shell.execute_reply.started": "2025-06-27T11:21:28.222156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: お元気ですか？\n",
      "AI: はい、元気です。人工知能のアシスタントとして、体の健康状態を気にする必要はありませんが、適切に作動していることが何よりも大切です。このように、あなたと対話できることを嬉しく思っています。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "chat_model = ResourceOptimizedChatBedrock(\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    client=bedrock_client\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"以下の質問にできるだけ正確に答えてください。\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "query=\"お元気ですか？\"\n",
    "response=wrapped_chain.invoke({\"input\": query})\n",
    "# Printing history to see the history being built out. \n",
    "print(history)\n",
    "# For the rest of the conversation, the output will only include response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新しい質問\n",
    "\n",
    "モデルは最初のメッセージで応答しました。次に、モデルにいくつか質問します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T11:21:36.561689Z",
     "iopub.status.busy": "2025-06-27T11:21:36.561309Z",
     "iopub.status.idle": "2025-06-27T11:21:52.708883Z",
     "shell.execute_reply": "2025-06-27T11:21:52.707922Z",
     "shell.execute_reply.started": "2025-06-27T11:21:36.561667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "始めてガーデニングをするなら、こんなヒントが役立つと思います。\n",
      "\n",
      "1. 日当たりと水はけの良い場所を選ぶ\n",
      "植物は十分な日光と適度な水分が必要です。日当たりと排水の良い場所を選びましょう。\n",
      "\n",
      "2. 地域に適した植物を選ぶ\n",
      "気候や土壌に合わない植物は育ちにくいので、地域に適した種類を選びましょう。園芸店や図書館で調べられます。\n",
      "\n",
      "3. 良質の土を用意する\n",
      "植物の健康な成長には良質の土が重要です。植木鉢用または庭園用の土を購入するのがおすすめです。\n",
      "\n",
      "4. 水やりの量と頻度に気をつける\n",
      "乾燥しすぎても湿りすぎても良くありません。土の表面を見ながら、適量と適度の頻度で水やりをしましょう。\n",
      "\n",
      "5. 雑草対策をする\n",
      "雑草は養分や水分を植物と奪い合うので、できるだけ早く取り除くことが大切です。\n",
      "\n",
      "6. 少しずつ始める\n",
      "ガーデニングは楽しい反面、手間もかかります。はじめは小規模から始め、徐々に広げていけば良いでしょう。\n",
      "\n",
      "楽しみながら少しずつ経験を積めば、どんどん上手になっていくはずです。頑張ってください!\n"
     ]
    }
   ],
   "source": [
    "#new questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"新しくガーデニングを始めるためのヒントをいくつか教えてください。\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 質問を基に構築\n",
    "\n",
    "次に、ガーデニングという言葉を使わずに質問をして、モデルが前の会話を理解できるかどうかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T11:21:54.909233Z",
     "iopub.status.busy": "2025-06-27T11:21:54.908928Z",
     "iopub.status.idle": "2025-06-27T11:22:07.526899Z",
     "shell.execute_reply": "2025-06-27T11:22:07.525863Z",
     "shell.execute_reply.started": "2025-06-27T11:21:54.909212Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "虫に関しても注意が必要ですね。\n",
      "\n",
      "1. 害虫を認識する\n",
      "アブラムシ、カメムシ、ハモグリバエなどの害虫を知り、早期に発見することが大切です。\n",
      "\n",
      "2. 有機的な防除を試す\n",
      "農薬を使わずに、石けん水やネームなどで防除を試みましょう。環境にも優しいですし、人体への影響も少ないでしょう。\n",
      "\n",
      "3. 天敵を活用する\n",
      "テントウムシやカブトムシなどの捕食性の昆虫は害虫の天敵です。有機農法で育てれば、天敵を呼び込むことができます。\n",
      "\n",
      "4. 手で取り除く\n",
      "少数の虫なら手で取り除くのも一つの方法です。虫が嫌いな方は手袋をするとよいでしょう。\n",
      "\n",
      "5. 最悪の場合は農薬を使用する\n",
      "有機的な方法でも防げない場合は、やむを得ず農薬を使用しますが、使用量や時期に気をつける必要があります。\n",
      "\n",
      "虫との闘いはガーデニングの重要な側面です。観察を怠らず、環境に配慮しながら対処していきましょう。\n"
     ]
    }
   ],
   "source": [
    "# build on the questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"虫についてはどうですか？\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### この会話を終える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T11:22:10.269735Z",
     "iopub.status.busy": "2025-06-27T11:22:10.269328Z",
     "iopub.status.idle": "2025-06-27T11:22:18.119208Z",
     "shell.execute_reply": "2025-06-27T11:22:18.117825Z",
     "shell.execute_reply.started": "2025-06-27T11:22:10.269710Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私も質問に答えられて良かったです。ガーデニングは少し手間はかかりますが、植物の成長を見守り、実りを収穫できる喜びもあります。初めてでも、基本を押さえて少しずつ経験を積めば、きっと上手くいくはずです。虫との付き合い方も大切なポイントですね。楽しみながら、試行錯誤を重ねていけば自分なりのコツが見つかるはずです。ガーデニングを始められて良かったですね。頑張ってくださいね。\n"
     ]
    }
   ],
   "source": [
    "# finishing the conversation\n",
    "instructions = [{\"role\": \"user\", \"content\": \"以上です、ありがとうございました！\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## タスク 4.3: プロンプトテンプレートを使用したチャットボット (LangChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一貫した対話のためのプロンプトテンプレートの使用\n",
    "\n",
    "プロンプトエンジニアリングは、大規模言語モデルを扱う上で重要な側面です。テンプレートを使用することで、プロンプトを一貫した方法で構造化し、異なるシナリオでモデルがどのように応答するかを制御できます。\n",
    "\n",
    "LangChain のプロンプトテンプレートは、異なる会話の役割（システム、ユーザー、アシスタント）に対する入力を標準化された方法でフォーマットし、モデルが適切に構造化されたコンテキストを受け取ることを保証します。\n",
    "\n",
    "このタスクでは、この入力の構築を担当するデフォルトの PromptTemplate を使用します。LangChain は、プロンプトの構築と操作を容易にするためのクラスと関数をいくつか提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T11:22:31.841890Z",
     "iopub.status.busy": "2025-06-27T11:22:31.841302Z",
     "iopub.status.idle": "2025-06-27T11:22:31.846532Z",
     "shell.execute_reply": "2025-06-27T11:22:31.845694Z",
     "shell.execute_reply.started": "2025-06-27T11:22:31.841825Z"
    }
   },
   "outputs": [],
   "source": [
    "#  prompt for a conversational agent\n",
    "def format_prompt(actor:str, input:str):\n",
    "    formatted_prompt: List[str] = []\n",
    "    if actor == \"system\":\n",
    "        prompt_template=\"\"\"<|begin_of_text|><|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    elif actor == \"user\":\n",
    "        prompt_template=\"\"\"<|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid role: {actor}. Role must be either 'user' or 'system'.\")   \n",
    "    prompt = PromptTemplate.from_template(prompt_template)     \n",
    "    formatted_prompt.extend(prompt.format(actor=actor,input=input))\n",
    "    formatted_prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T11:23:55.729154Z",
     "iopub.status.busy": "2025-06-27T11:23:55.728846Z",
     "iopub.status.idle": "2025-06-27T11:23:55.737179Z",
     "shell.execute_reply": "2025-06-27T11:23:55.736243Z",
     "shell.execute_reply.started": "2025-06-27T11:23:55.729131Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat user experience\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            with self.out:\n",
    "                print(\"ありがとう、楽しい会話でした!!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        response = self.qa.invoke({\"input\": prompt})\n",
    "                        result=response['answer']\n",
    "                    else:\n",
    "                        instructions = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                        #result = self.qa.invoke({'input': format_prompt(\"user\",prompt)}) #, 'history':chat_history})\n",
    "                        result = self.qa.invoke({\"input\": format_instructions(instructions)})\n",
    "                except:\n",
    "                    result = \"回答なし\"\n",
    "                thinking.value=\"\"\n",
    "                print(f\"AI:{result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You:\", placeholder='q で終了します')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、チャットを開始します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T11:24:03.972345Z",
     "iopub.status.busy": "2025-06-27T11:24:03.971720Z",
     "iopub.status.idle": "2025-06-27T11:24:03.993682Z",
     "shell.execute_reply": "2025-06-27T11:24:03.992449Z",
     "shell.execute_reply.started": "2025-06-27T11:24:03.972314Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0c4f3e69c34b9bb8a19b39c43f4d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e810ba987e7486fa29b003a3793701c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Text(value='', description='You:', placeholder='q で終了します'), Button(description='Send', style=But…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start chat\n",
    "history = InMemoryChatMessageHistory() #reset chat history\n",
    "chat = ChatUX(wrapped_chain)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## タスク 4.4: ペルソナ付きチャットボット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このタスクでは、人工知能 (AI) アシスタントがキャリアコー​​チの役割を果たします。システムメッセージを使用して、チャットボットにペルソナ (または役割) を通知できます。会話のコンテキストを維持するために、InMemoryChatMessageHistory クラスを引き続き活用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたはキャリアコーチとして行動します。あなたの目標は、ユーザーにキャリアに関するアドバイスを提供することです。キャリアに関係のない質問に対しては、アドバイスを提供しないでください。「わかりません」と答えてください。\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory() # reset history\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"career_chat_history\",\n",
    ")\n",
    "\n",
    "response=wrapped_chain.invoke({\"input\": \"AI におけるキャリアの選択肢は何ですか?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=wrapped_chain.invoke({\"input\": \"車を修理するにはどうすればいいですか?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、このペルソナの専門分野に属さない質問をします。モデルはその質問には答えず、その理由を説明する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## タスク 4.5 コンテキスト付きチャットボット\n",
    "\n",
    "このタスクでは、渡されたコンテキストに基づいてチャットボットに質問に答えるように依頼します。CSV ファイルを取得し、Titan 埋め込みモデルを使用してそのコンテキストを表すベクトルを作成します。このベクトルは Facebook AI 類似性検索 (FAISS) に保存されます。チャットボットに質問が出されたら、このベクトルをチャットボットに返し、ベクトルを使用して回答を取得させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titan 埋め込みモデル\n",
    "\n",
    "埋め込みは、単語、フレーズ、またはその他の離散項目を連続ベクトル空間内のベクトルとして表します。これにより、機械学習モデルはこれらの表現に対して数学的演算を実行し、それらの間の意味関係をキャプチャできます。\n",
    "\n",
    "埋め込みは、検索拡張生成 (RAG) [ドキュメント検索機能](https://labelbox.com/blog/how-vector-similarity-search-works/) に使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model configuration\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "br_embeddings = ResourceOptimizedEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\",\n",
    "    client=bedrock_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorStore としての FAISS\n",
    "\n",
    "検索に埋め込みを使用するには、ベクトル類似性検索を効率的に実行できるストアが必要です。このノートブックでは、インメモリストアである FAISS を使用します。ベクトルを永続的に保存するには、Amazon Bedrock、pgVector、Pinecone、Weaviate、または Chroma の Knowledge Base を使用できます。\n",
    "\n",
    "LangChain VectorStore API は [こちら](https://python.langchain.com/v0.2/docs/integrations/vectorstores/) から入手できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vector store\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "loader = CSVLoader(\"./Amazon_SageMaker_FAQs_jp.csv\")\n",
    "documents_aws = loader.load() #\n",
    "print(f\"documents:loaded:size={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Documents:after split and chunking size={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "try:\n",
    "    \n",
    "    vectorstore_faiss_aws = FAISS.from_documents(\n",
    "        documents=docs,\n",
    "        embedding = br_embeddings, \n",
    "        #**k_args\n",
    "    )\n",
    "\n",
    "    print(f\"vectorstore_faiss_aws:created={vectorstore_faiss_aws}::\")\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 簡単なローコード テストを実行する\n",
    "\n",
    "LangChain が提供する Wrapper クラスを使用して、ベクター データベース ストアをクエリし、関連するドキュメントを返すことができます。これにより、すべてのデフォルト値で QA チェーンが実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_llm=ResourceOptimizedChatBedrock(\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    client=bedrock_client)\n",
    "# wrapper store faiss\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)\n",
    "print(wrapper_store_faiss.query(\"SageMakerにおけるR\", llm=chat_llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### チャットボット アプリケーション\n",
    "\n",
    "チャットボットには、コンテキスト管理、履歴、ベクターストア、その他多くのコンポーネントが必要です。まず、コンテキストをサポートする Retrieval Augmented Generation (RAG) チェーンを構築します。\n",
    "\n",
    "これには、**create_stuff_documents_chain** 関数と **create_retrieval_chain** 関数が使用されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG に使用されるパラメータ\n",
    "\n",
    "- **Retriever:** `VectorStore` を基盤とする `VectorStoreRetriever` を使用しました。テキストを取得するには、`\"similarity\"` または `\"mmr\"` の 2 つの検索タイプから選択できます。`search_type=\"similarity\"` は、リトリーバーオブジェクトで類似性検索を使用し、質問ベクトルに最も類似するテキストチャンクベクトルを選択します。\n",
    "\n",
    "- **create_stuff_documents_chain** は、取得したコンテキストをプロンプトと LLM に取り込む方法を指定します。取得したドキュメントは、要約やその他の処理をプロンプトに施すことなく、コンテキストとして「詰め込まれ」ます。\n",
    "\n",
    "- **create_retrieval_chain** は、取得ステップを追加し、取得したコンテキストをチェーンに伝播して、最終的な回答とともに提供します。\n",
    "\n",
    "質問がコンテキストの範囲外である場合、モデルは答えがわからないと応答します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"あなたは質問回答タスクのアシスタントです。\"\n",
    "    \"取得した下記のコンテキスト部分を使用して質問に回答します。 \"\n",
    "    \"答えが分からない場合は、 「わかりません」と回答して下さい。 \"\n",
    "    \"最大3文で簡潔に答えてください。 \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever=vectorstore_faiss_aws.as_retriever()\n",
    "question_answer_chain = create_stuff_documents_chain(chat_llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"SageMaker とは何ですか？\"})\n",
    "print(response) # shows the document chunks consulted to come up with the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にチャットを開始します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatUX(rag_chain, retrievalChain=True)\n",
    "chat.start_chat()  # Only answers will be shown here, and not the citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude LLM を利用して、次のパターンで会話型インターフェースを作成しました:\n",
    "\n",
    "- チャットボット (基本 - コンテキストなし)\n",
    "- プロンプトテンプレート (Langchain) を使用したチャットボット\n",
    "- ペルソナ付きチャットボット\n",
    "- コンテキスト付きチャットボット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 試してみましょう\n",
    "\n",
    "- 特定のユースケースに合わせてプロンプトを変更し、さまざまなモデルの出力を評価します。\n",
    "- トークンの長さを変えることで、サービスのレイテンシと応答性がどのように変化するかを理解します。\n",
    "- さまざまなプロンプトエンジニアリングの原則を適用して、より良い出力を取得します。\n",
    "\n",
    "### クリーンアップ\n",
    "\n",
    "あなたはこのノートブックを完了しました。ラボの次のパートに移るには、下記を実行してください。:\n",
    "\n",
    "- このノートブックファイルを閉じ、**タスク 5** に進んでください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

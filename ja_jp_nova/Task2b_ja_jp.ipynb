{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fded102b",
   "metadata": {},
   "source": [
    "# タスク 2b: 抽象的なテキスト要約\n",
    "\n",
    "このノートブックでは、大規模なドキュメント要約で発生する課題に対処します。入力テキストがモデルのコンテキスト長を超えたり、ハルシネーションを生成したり、メモリ不足エラーを引き起こしたりする可能性があります。\n",
    "\n",
    "これらの問題を軽減するために、このノートブックでは、言語モデルを活用するアプリケーションを可能にするツールキットである [LangChain](https://python.langchain.com/docs/get_started/introduction.html) フレームワークを使用してプロンプトのチャンク化と連鎖化を使用するアーキテクチャを示します。\n",
    "\n",
    "ユーザー ドキュメントがトークン制限を超えた場合のシナリオに対処するアプローチを探ります。チャンク化では、ドキュメントをコンテキスト長のしきい値以下のセグメントに分割してから、モデルに順番に入力します。これにより、プロンプトがチャンク間で連鎖され、以前のコンテキストが保持されます。このアプローチを適用して、通話のトランスクリプト、会議のトランスクリプト、書籍、記事、ブログ投稿、その他の関連コンテンツを要約します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1eaf9",
   "metadata": {},
   "source": [
    "## Task 2b.1: 環境のセットアップ\n",
    "\n",
    "このタスクでは、環境をセットアップし、AWS リージョンを自動的に検出する Bedrock クライアントを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f9067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "# AWS and Bedrock imports\n",
    "import boto3\n",
    "\n",
    "# Get the region programmatically\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name or \"us-east-1\"  # Default to us-east-1 if region not set\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae9a41",
   "metadata": {},
   "source": [
    "## タスク 2b.2: 長いテキストを要約する\n",
    "\n",
    "### Boto3 を使用した LangChain の構成\n",
    "\n",
    "このタスクでは、LangChain Bedrock クラスの LLM を指定し、推論用の引数を渡すことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df2442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LangChain imports\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "# Base LLM configuration\n",
    "modelId = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "class NovaLiteWrapper(LLM):\n",
    "    \"\"\"Wrapper for Nova Lite model that formats inputs correctly.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"nova-lite-wrapper\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Format prompt for Nova Lite and process.\"\"\"\n",
    "        # Format the prompt for Nova Lite's expected message structure\n",
    "        formatted_input = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"text\": prompt}]  # Content must be an array with text objects\n",
    "                }\n",
    "            ],\n",
    "            \"inferenceConfig\": {\n",
    "                \"maxTokens\": 2048,\n",
    "                \"temperature\": 0,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Call Bedrock directly with the properly formatted input\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=modelId,\n",
    "            body=json.dumps(formatted_input)\n",
    "        )\n",
    "        \n",
    "        # Parse the response - updated to handle Nova Lite's response format\n",
    "        response_body = json.loads(response['body'].read().decode('utf-8'))\n",
    "        \n",
    "        # Extract the text from the response\n",
    "        if 'output' in response_body and 'message' in response_body['output']:\n",
    "            message = response_body['output']['message']\n",
    "            if 'content' in message and isinstance(message['content'], list):\n",
    "                # Extract text from each content item\n",
    "                texts = []\n",
    "                for content_item in message['content']:\n",
    "                    if isinstance(content_item, dict) and 'text' in content_item:\n",
    "                        texts.append(content_item['text'])\n",
    "                return ' '.join(texts)\n",
    "        \n",
    "        # Fallback if the response format is different\n",
    "        return str(response_body)\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model_id\": modelId}\n",
    "    \n",
    "    def get_num_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate token count - Nova Lite uses roughly 1 token per 4 characters.\"\"\"\n",
    "        return len(text) // 4  # Rough approximation\n",
    "\n",
    "# Create the Nova Lite wrapper\n",
    "llm = NovaLiteWrapper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d423aa-cb67-4170-afdb-b037f2531921",
   "metadata": {},
   "source": [
    "## リソース最適化された LLM ラッパーの作成\n",
    "\n",
    "Bedrock のサービスクォータを効果的に処理するために、リソース使用量を最適化し、API 呼び出しにジッター付きの指数バックオフを実装するラッパークラスを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87119b-bf9d-4bec-be54-1efbcc9e8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced resource-optimized LLM wrapper with exponential backoff\n",
    "class ResourceOptimizedLLM(LLM):\n",
    "    \"\"\"Wrapper that optimizes resource usage for LLM processing.\"\"\"\n",
    "    \n",
    "    llm: Any  # The base LLM to wrap\n",
    "    min_pause: float = 30.0  # Minimum pause between requests\n",
    "    max_pause: float = 60.0  # Maximum pause after throttling\n",
    "    initial_pause: float = 10.0  # Initial pause between requests\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return f\"optimized-{self.llm._llm_type}\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Process with resource optimization and exponential backoff.\"\"\"\n",
    "        # Always pause between requests to optimize resource usage\n",
    "        time.sleep(self.initial_pause)\n",
    "        \n",
    "        # Implement retry with exponential backoff\n",
    "        max_retries = 10  # More retries for important operations\n",
    "        base_delay = self.min_pause\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Making API call (attempt {attempt+1}/{max_retries})...\")\n",
    "                return self.llm._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                \n",
    "                # Handle different types of service exceptions\n",
    "                if any(err in error_str for err in [\"ThrottlingException\", \"TooManyRequests\", \"Rate exceeded\"]):\n",
    "                    if attempt < max_retries - 1:\n",
    "                        # Calculate backoff with jitter to prevent request clustering\n",
    "                        jitter = random.random() * 0.5\n",
    "                        wait_time = min(base_delay * (2 ** attempt) + jitter, self.max_pause)\n",
    "                        \n",
    "                        print(f\"Service capacity reached. Backing off for {wait_time:.2f} seconds...\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(\"Maximum retries reached. Consider reducing batch size or increasing delays.\")\n",
    "                        raise\n",
    "                else:\n",
    "                    # For non-capacity errors, don't retry\n",
    "                    print(f\"Non-capacity error: {error_str}\")\n",
    "                    raise\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {**self.llm._identifying_params, \"initial_pause\": self.initial_pause}\n",
    "    \n",
    "    def get_num_tokens(self, text: str) -> int:\n",
    "        \"\"\"Pass through token counting to the base model.\"\"\"\n",
    "        return self.llm.get_num_tokens(text)\n",
    "\n",
    "# Create the resource-optimized LLM\n",
    "resource_optimized_llm = ResourceOptimizedLLM(llm=llm, initial_pause=10.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b8886-b2e9-4239-8556-1f1483e9bfef",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **注:** このラッパーは、本番環境での使用に重要な機能を追加します。\n",
    "\n",
    "- サービスクォータを遵守するためのリクエスト間の自動一時停止\n",
    "- スロットリング例外を処理するためのジッター付き指数バックオフ\n",
    "- 包括的なエラー処理とレポート"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31223056",
   "metadata": {},
   "source": [
    "## タスク 2b.3: 多数のトークンを含むテキスト ファイルの読み込み\n",
    "\n",
    "このタスクでは、letters ディレクトリに [Amazon の CEO による 2022 年の株主への手紙](https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-2022-letter-to-shareholders) のコピーを使用します。テキストファイルを読み込み、発生する可能性のあるエラーを処理する関数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70352ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Document loading function\n",
    "def load_document(file_path):\n",
    "    \"\"\"Load document from file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "shareholder_letter = \"./2022-letter-jp.txt\"\n",
    "letter = load_document(shareholder_letter)\n",
    "\n",
    "if letter:\n",
    "    num_tokens = resource_optimized_llm.get_num_tokens(letter)\n",
    "    print(f\"Document loaded successfully with {num_tokens} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0e622",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **注:** 警告は無視して次のセルに進んでください。この問題は、次の手順でドキュメントをチャンク化することで解決します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ec39d",
   "metadata": {},
   "source": [
    "## タスク 2b.4: 長いテキストをチャンクに分割する\n",
    "\n",
    "このタスクでは、プロンプトに収まりきらないほど長いテキストを小さなチャンクに分割します。LangChainの`RecursiveCharacterTextSplitter`は、各チャンクのサイズがchunk_sizeよりも小さくなるまで、長いテキストを再帰的にチャンクに分割することをサポートしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c372b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Document chunking with conservative settings\n",
    "def chunk_document(text, chunk_size=4000, chunk_overlap=200):\n",
    "    \"\"\"Split document into manageable chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.create_documents([text])\n",
    "    print(f\"Document split into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# Split the document into chunks\n",
    "if letter:\n",
    "    docs = chunk_document(letter, chunk_size=4000, chunk_overlap=200)\n",
    "    \n",
    "    if docs:\n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = resource_optimized_llm.get_num_tokens(docs[0].page_content)\n",
    "        print(f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acedb37-52f7-40ac-ae70-cede47b270a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **注:** `chunk_size` パラメータは、各チャンクのサイズを制御します。チャンクが大きいほどコンテキストは多くなりますが、処理リソースも必要になります。`chunk_overlap` パラメータは、チャンク間の連続性を確保します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8ae45",
   "metadata": {},
   "source": [
    "## タスク 2b.5: チャンクを要約して結合する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d49f5",
   "metadata": {},
   "source": [
    "このタスクでは、分割されたドキュメントを要約するための2つのアプローチを実装します。LangChain の組み込み要約チェーンを使用する方法と、リソース使用量をより細かく制御できるカスタムの手動実装を使用する方法です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc48123-d9f4-4275-a509-4ed0a687d547",
   "metadata": {},
   "source": [
    "## 実装アプローチの理解\n",
    "\n",
    "このノートブックでは、AWS Bedrockを使用して大規模な文書を要約する2つの異なるアプローチを説明します。\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **注:** 本番アプリケーションを構築する際の利便性と制御のトレードオフを示すため、標準的なLangChain実装とカスタム実装の両方を含めています。\n",
    "\n",
    "### 同じ目標への 2 つの道筋\n",
    "\n",
    "1. **標準的な LangChain 実装*** (`process_documents_with_pacing`):\n",
    "   - LangChain の組み込み要約チェーンを使用\n",
    "   - より少ないコードで実装が容易\n",
    "   - 基礎となる複雑さを抽象化\n",
    "   - 迅速なプロトタイピングと単純なユースケースに最適\n",
    "\n",
    "2. **カスタム改良実装** (`manual_refine_with_optimization`):\n",
    "   - 改良プロセスをステップバイステップで構築\n",
    "   - プロンプトと処理の完全な可視性を提供\n",
    "   - 各文書チャンクに対する詳細なエラー処理\n",
    "   - API コールのタイミングとリトライロジックの正確な制御が可能\n",
    "\n",
    "両者は同じ最終結果を達成しますが、カスタム実装では、サービスクォータを扱い、本番環境ready向けのアプリケーションを構築する際に重要となる、プロセス全体のより詳細な制御が可能です。\n",
    "\n",
    "実際のシナリオでは、開発中は標準実装から始め、リソース使用量、エラー処理、またはプロンプトエンジニアリングのより詳細な制御が必要になった時点でカスタム実装に移行することがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a2fa5-a989-4330-ad46-72ddc631b7b8",
   "metadata": {},
   "source": [
    "### 標準的な LangChain の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f7535-31cf-4809-ba1b-a7ef5e3196a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# カスタムドキュメント処理と制御されたペーシング\n",
    "def process_documents_with_pacing(docs, chain_type=\"refine\", verbose=True):\n",
    "    \"\"\"Process documents with pacing to optimize resource usage.\"\"\"\n",
    "    \n",
    "    # チェーンの設定\n",
    "    summary_chain = load_summarize_chain(\n",
    "        llm=resource_optimized_llm,\n",
    "        chain_type=chain_type,  # \"refine\" processes sequentially, good for resource optimization\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # 追加のエラーハンドリングを伴う処理\n",
    "    try:\n",
    "        result = summary_chain.invoke(docs)\n",
    "        return result\n",
    "    except ValueError as error:\n",
    "        if \"AccessDeniedException\" in str(error):\n",
    "            print(f\"\\n\\033[91mAccess Denied: {error}\\033[0m\")\n",
    "            print(\"\\nTo troubleshoot this issue, please check:\")\n",
    "            print(\"1. Your IAM permissions for Bedrock\")\n",
    "            print(\"2. Model access permissions\")\n",
    "            print(\"3. AWS credentials configuration\")\n",
    "            return {\"output_text\": \"Error: Access denied. Check permissions.\"}\n",
    "        else:\n",
    "            print(f\"\\n\\033[91mError during processing: {error}\\033[0m\")\n",
    "            return {\"output_text\": f\"Error during processing: {str(error)}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988cc73-0982-4b69-88cb-7066176d29b5",
   "metadata": {},
   "source": [
    "### リソース最適化を強化したカスタムリファインの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216651f-0fc4-42e6-93c8-8adf65b729a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リファインチェーンのためのリソース最適化処理の手動実装\n",
    "def manual_refine_with_optimization(docs, llm, verbose=True):\n",
    "    \"\"\"Manually implement refine chain with resource optimization.\"\"\"\n",
    "    if not docs:\n",
    "        return {\"output_text\": \"No documents to process.\"}\n",
    "    \n",
    "    # 最初のドキュメントを処理して初期要約を取得\n",
    "    print(f\"Processing initial document (1/{len(docs)})...\")\n",
    "    \n",
    "    # 初期ドキュメント用のシンプルなプロンプト\n",
    "    initial_prompt = \"\"\"Write a concise summary of the following:\n",
    "    \"{text}\"\n",
    "    CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "    # 最初のドキュメントを処理\n",
    "    try:\n",
    "        current_summary = llm(initial_prompt.format(text=docs[0].page_content))\n",
    "        print(\"Initial summary created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating initial summary: {e}\")\n",
    "        return {\"output_text\": \"Failed to create initial summary.\"}\n",
    "    \n",
    "    # 残りのドキュメントをリファインアプローチで処理\n",
    "    for i, doc in enumerate(docs[1:], start=2):\n",
    "        print(f\"Refining with document {i}/{len(docs)}...\")\n",
    "        \n",
    "        # リファインプロンプト\n",
    "        refine_prompt = \"\"\"既存の要約を改良してください。\n",
    "        既存の要約は次のとおりです: {existing_summary}\n",
    "\n",
    "        情報を追加する新しい文書があります: {text}\n",
    "\n",
    "        文書からの新しい情報を取り入れて要約を更新してください。\n",
    "        文書に関連情報が含まれていない場合は、既存の要約を返してください。\n",
    "\n",
    "        改良された要約:\"\"\"\n",
    "                \n",
    "        try:\n",
    "            # Apply resource optimization between requests\n",
    "            time.sleep(10.0)  # Base delay between requests\n",
    "            \n",
    "            # Update the summary\n",
    "            current_summary = llm(refine_prompt.format(\n",
    "                existing_summary=current_summary,\n",
    "                text=doc.page_content\n",
    "            ))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Successfully refined with document {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during refinement with document {i}: {e}\")\n",
    "            # Apply exponential backoff\n",
    "            backoff = min(10.0 * (2 ** (i % 5)) + (random.random() * 2), 30)\n",
    "            print(f\"Backing off for {backoff:.2f} seconds...\")\n",
    "            time.sleep(backoff)\n",
    "            \n",
    "            # Try one more time\n",
    "            try:\n",
    "                current_summary = llm(refine_prompt.format(\n",
    "                    existing_summary=current_summary,\n",
    "                    text=doc.page_content\n",
    "                ))\n",
    "            except Exception as retry_error:\n",
    "                print(f\"Retry failed for document {i}: {retry_error}\")\n",
    "                # Continue with current summary rather than failing completely\n",
    "    \n",
    "    return {\"output_text\": current_summary}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d2892-54d2-43a1-a69c-58a45432cfe1",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **注:** 手動で実装すると、次の点をより細かく制御できます。\n",
    "\n",
    "- 要約に使用された正確なプロンプト\n",
    "- エラー処理とリカバリー\n",
    "- API 呼び出し間のリソース最適化\n",
    "- エラー発生時のグレースフルな機能低下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c914f0-cbc3-48b4-a9ee-f14d02ce921e",
   "metadata": {},
   "source": [
    "## タスク 2b.6: メイン実行関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e4d11-a876-4bd9-9d91-404017452916",
   "metadata": {},
   "source": [
    "ここで、ドキュメント要約プロセス全体を調整するメイン関数を作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819efba-6a3f-4aaa-8668-13b45993af10",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **注:** メイン関数 (`summarize_document`) を使用すると、`chain_type` パラメータに基づいて使用する実装を選択できるため、結果とパフォーマンスを簡単に比較できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d0718-2adf-4bf9-bf18-acf4644115d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# メイン実行関数\n",
    "def summarize_document(file_path, chunk_size=4000, chain_type=\"refine\"):\n",
    "    \"\"\"Main function to summarize a document.\"\"\"\n",
    "    \n",
    "    print(f\"Starting document summarization process for: {file_path}\")\n",
    "    \n",
    "    # 文書の読み込み\n",
    "    document_text = load_document(file_path)\n",
    "    if not document_text:\n",
    "        return \"Failed to load document.\"\n",
    "    \n",
    "    print(f\"Document loaded successfully. Length: {len(document_text)} characters\")\n",
    "    \n",
    "    # チャンクに分割\n",
    "    docs = chunk_document(document_text, chunk_size=chunk_size, chunk_overlap=200)\n",
    "    \n",
    "    # If document is very large, provide a warning\n",
    "    if len(docs) > 15:\n",
    "        print(f\"Warning: Document is large ({len(docs)} chunks). Processing may take some time.\")\n",
    "        \n",
    "        # For very large documents, consider using a subset for testing\n",
    "        if len(docs) > 30:\n",
    "            print(\"Document is extremely large. Consider using a smaller chunk_size or processing a subset.\")\n",
    "            # Optional: process only a subset for testing\n",
    "            # docs = docs[:15]\n",
    "    \n",
    "    # ドキュメントを処理\n",
    "    print(f\"Processing document using '{chain_type}' chain type...\")\n",
    "    \n",
    "    # チェーンタイプに基づいて適切な処理方法を使用\n",
    "    if chain_type == \"refine\":\n",
    "        # リソース最適化のより良い制御のために手動実装を使用\n",
    "        result = manual_refine_with_optimization(docs, resource_optimized_llm)\n",
    "    else:\n",
    "        # 他のチェーンタイプには標準のLangChain実装を使用\n",
    "        result = process_documents_with_pacing(docs, chain_type=chain_type)\n",
    "    \n",
    "    # 結果を返す\n",
    "    if result and \"output_text\" in result:\n",
    "        print(\"\\nSummarization completed successfully!\")\n",
    "        return result[\"output_text\"]\n",
    "    else:\n",
    "        print(\"\\nSummarization failed or returned no result.\")\n",
    "        return \"Summarization process did not produce a valid result.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcfca15-b18e-4aba-8a09-2500fc941f8e",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **注:** ドキュメントの数、Bedrock 要求レート クォータ、および構成された再試行設定によっては、要約プロセスの実行に時間がかかる場合があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a31741-103d-4524-b3a8-3c68da2baa07",
   "metadata": {},
   "source": [
    "## タスク 2b.7: 要約を実行する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0de4dd-9430-46fe-ae9f-8af121f41b51",
   "metadata": {},
   "source": [
    "株主レターの要約を実行してみましょう。デフォルトでは、summary_document() 関数は Refinance チェーンを使用します。map_reduce を有効にするには、以下の手順に従います。\n",
    "\n",
    "- 次の行をコメントアウトします: `summary = summarize_document(document_path, chunk_size=4000, chain_type=\"refine\")`\n",
    "- 次の行をコメント解除します: `# summary = summarize_document(document_path, chunk_size=4000, chain_type=\"map_reduce\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff42a88c-44af-4b35-8eed-d8ecd3ea38df",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **注:** 実行中にエラーメッセージが表示されても心配しないでください。コードには堅牢なエラー処理が含まれており、失敗したリクエストは指数バックオフで自動的に再試行されます。これはサービスクォータを使用する場合の正常な動作であり、本番環境対応アプリケーションがAPI制限をどのように処理すべきかを示しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ffd26-bfbd-47d1-b6d7-76daabc8e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    # ドキュメントへのパス\n",
    "    document_path = \"./2022-letter-jp.txt\"\n",
    "    \n",
    "    # 異なるオプションで要約\n",
    "    # オプション1: 標準的なリファインチェーン（シーケンシャル処理、リソース最適化に適している）\n",
    "    summary = summarize_document(document_path, chunk_size=4000, chain_type=\"refine\")\n",
    "    \n",
    "    # オプション2: 比較用のmap_reduce（ただしサービスクォータに注意）\n",
    "    # summary = summarize_document(document_path, chunk_size=4000, chain_type=\"map_reduce\")\n",
    "    \n",
    "    # 最終要約の出力\n",
    "    print(\"\\n=== 最終要約 ===\\n\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21988a-db39-4511-a426-b21feb75d2f2",
   "metadata": {},
   "source": [
    "これで、LangChainフレームワークを使用したプロンプトチャンキングとチェーニングを実験し、長文入力テキストから生じる問題を軽減しながら大きなドキュメントを要約することができました。\n",
    "\n",
    "## 主要コンポーネントの理解\n",
    "\n",
    "私たちのソリューションの主要コンポーネントを見直してみましょう：\n",
    "\n",
    "1. **リソース最適化**: **ResourceOptimizedLLM**ラッパーは以下によってBedrockサービスクォータ内でAPIコールを管理します：\n",
    "   - リクエスト間に一時停止を追加（**initial_pause**によって制御）\n",
    "   - スロットリング発生時のジッター付き指数バックオフの実装\n",
    "   - 包括的なエラー処理とリカバリの提供\n",
    "\n",
    "2. **ドキュメントチャンキング**: **chunk_document**関数は大きなドキュメントを管理可能な部分に分割します：\n",
    "   - **chunk_size** は各チャンクの最大サイズを制御（4000文字）\n",
    "   - **chunk_overlap** はチャンク間のコンテキストの連続性を確保（200文字）\n",
    "   - 自然なテキストセパレータ（**\\n\\n**, **\\n**, **.** など）を使用して段落の途中での分割を回避\n",
    "\n",
    "3. **要約アプローチ**:\n",
    "   - **リファインチェーン**: チャンクを順次処理し、新しいチャンクごとに要約を改良\n",
    "   - **Map-Reduce**: 各チャンクを独立して要約し、それらの要約を結合して要約\n",
    "\n",
    "4. **エラー処理**: 包括的なエラー処理により、以下からの回復が可能：\n",
    "   - サービスのスロットリングと容量制限\n",
    "   - アクセス権限の問題\n",
    "   - その他のAPIエラー\n",
    "\n",
    "## 自分で試してみる\n",
    "\n",
    "- プロンプトを特定のユースケースに変更し、異なるモデルの出力を評価する\n",
    "- 異なるチャンクサイズを試して、コンテキスト保持と処理効率のバランスの最適値を見つける\n",
    "- 異なる要約チェーンタイプ（**refine**対**map_reduce**）を試して結果を比較する\n",
    "- Bedrockクォータ制限に基づいてリソース最適化パラメータを調整する\n",
    "\n",
    "### 実践的な応用\n",
    "\n",
    "このアプローチは以下のような様々な長文コンテンツの要約に適用できます：\n",
    "- カスタマーサービスの通話トランスクリプト\n",
    "- 会議のトランスクリプトとノート\n",
    "- 研究論文と技術文書\n",
    "- 法的文書と契約書\n",
    "- 書籍、記事、ブログ投稿\n",
    "\n",
    "### ベストプラクティス\n",
    "\n",
    "このソリューションを本番環境に実装する際：\n",
    "\n",
    "1. **API使用状況のモニタリング**: クォータ制限内に収まるようにAPIコールを追跡\n",
    "2. **チャンクサイズの最適化**: コンテキスト保持と処理効率のバランスを取る\n",
    "3. **適切なエラー処理の実装**: アプリケーションがAPIエラーを適切に処理できるようにする\n",
    "4. **キャッシュの検討**: 頻繁にアクセスされるドキュメントの冗長なAPIコールを避けるためにキャッシュを使用\n",
    "5. **様々な文書タイプでのテスト**: 異なるコンテンツには異なるチャンキング戦略が必要な場合がある\n",
    "\n",
    "### クリーンアップ\n",
    "\n",
    "このノートブックを完了しました。ラボの次の部分に進むには、以下を実行してください：\n",
    "\n",
    "- このノートブックファイルを閉じて、**タスク 3 **に進んでください。"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
